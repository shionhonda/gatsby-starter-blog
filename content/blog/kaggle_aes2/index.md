---
title: "Kaggle Competition Report: Automated Essay Scoring 2.0"
date: "2024-07-19T20:01:03.284Z"
description: "Can LLMs answer scientific questions? See how Kaggle winners used LLMs and RAG!"
featuredImage: kaggle_aes2/ogp.jpg
tags: ["en", "kaggle", "nlp"]
---

The Kaggle competition "Learning Agency Lab - Automated Essay Scoring 2.0" wrapped up on July 3rd, attracting over 2,600 teams.

Though I wasn't among the competitors, I'm going to create a retrospective analysis on the winners' solutions, as they offer valuable insights into how we use LLMs for answering questions based on certain contexts.

## Problem Setting

In this competition, the task was to answer science questions written by an LLM, specifically GPT-3.5, on a topic drawn from Wikipedia. Each question presented five possible choices. Participants had to rank the five choices in the descending order of correctness. Then the submissions were evaluated based on **mean average precision** (**MAP**) @5.

This competition is interesting in 3 ways:

- Due to the Kaggle Notebook's constraints, the largest model that can run in a ordinary setting waas around 10B. So, this competition could be seen as a problem of using smaller LLMs to answer questions from larger LLMs
- The simplest approach to this problem was to use large text models for classification. But there was a much more promising approach, **retrieval augmented generation** (**RAG**). This is, as you may know, the hottest topic in the industry nowadays
- The provided train set contained only 400 questions, meaning that crafting high-quality datasets (for training and for retrieval) could be a key to winning

## Final Standings

The final standings are in the image below.

![alt text](image.png)

![alt text](image-1.png)
https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/516579

## Solutions

Let's have a look the solutions of the top five teams.

### 1st Place

https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/516791

[Team H2O LLM Studio](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446422) secured the first place by making the full use of LLMs and RAG.

In the retrieval phase, the team diversified their knowledge sources by using different Wikipedia dumps. An attempt to filter the sources for science articles did not work because the retrieval models were robust enough to ignore irrelevant documents. They incorporated different models from [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) and embedded the concatenations of title and article chunk. The retrieval was done by a simple, but scalablly implemented PyTorch matrix multiplication.

For training data, they relied on [@radek1's 6.5k samples generated by GPT-3.5](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam). On this dataset, they fine-tuned all the linear layers of LLMs (7B - 13B) with LoRA, using a binary classification approach. Each model took as input a concatenation of retrieved contexts, question, and one of the possible answers, and it predicted the likelihood of the answer being correct. During the inference time, this prediction was repeated for all the five choices and the they were sorted in the order of the predicted score. It is worth noting that during this process, [the context and question was cached as `past_key_values`](https://discuss.huggingface.co/t/past-key-values-why-not-past-key-values-queries/31941).

They created five 7B models and one 13B model with different configurations (data source, embedding model, and the number of retrieved chunks) and ensembled the outputs.

### 2nd Place

https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/516790

https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/516582

Solo competitor [@solokin](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/448256) clinched the runner-up spot by blending traditional and cutting-edge methods.

The retrieval system utilized [graelo/wikipedia/20230601.en](https://huggingface.co/datasets/graelo/wikipedia/viewer/20230601.en) dataset as a single source of knowledge. Documents were segmented into sentences and organized into overlapping chunks, which then were indexed using [Apache Lucene's **BM-25** algorithm](https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html). Retrieved chunks were reordered by a DeBERTa v3 reranker when composing a prompt.

For training, they first curated a larger dataset than [@radek1's](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam). Then they trained models (DeBERTa v3 and Mistral) with a multi-class classification objective.

The outputs of these models were mixed by a custom XGBRanker.

### 3rd Place

https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/516631

[@podpall](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446358) created a huge pipeline including many small LMs and even two 70B models! I won't go into the details of this, but it's incredible to manage to run this system on a Kaggle's notebook.

![huge pipeline](podpall.png)

### 4th Place

https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/516639

[üìù Preferred Scantron üìù](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446307) earned fourth place without using billion-parameter models for answering questions.

The team used Elasticsearch for sentence-wise keyword retrieval. They reranked the results via different methods: Elasticsearch score, edit distance score, and semantic search score. This system was tested in a zero-shot manner using a Llama 2 7B model and a subset of [@radek1's questions](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam).

They trained a DeBERTa v3 Large model (~300M parameters) on token lengths up to 1280 tokens. As they increased tokens from 512 to 1280, their public score steadily increased.

The ensemble strategy involved using multiple contexts reranked by different metrics.

### 5th Place

[Preferred „Åä„Åó„ÇÉ„Åπ„Çä„Çì„Åº„ÅÜ(ChattyKids)](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446293) took the fifth prize with a 70B model and a 3-stage inference strategy.

For finding the right information, they leaned on [pyserini](https://github.com/castorini/pyserini)'s implementation of BM25. They also implemented dense retrieval by embedding sentences and paragraphs from the Wikipedia dump.

In the modeling phase, they used Mistral 7B and Llama 2 70B, with a focus on non-instruction tuned models. They finetuned models for a multi-class classification objective using QLoRA with 4-bit quantization and [xFormers](https://github.com/facebookresearch/xformers)' `memory_efficient_attention` to manage GPU memory constraints.

Finally, they adopted a unique 3-stage inference pipeline, where smaller models like Mistral 7B answer easy problems and larger models like Llama 2 70B answer difficult problems. This way, they made full use of the limited inference time.

## My Experience

I had trouble in the computing environment ([which was resolved at the very end of the competition](https://hippocampus-garden.com/workbench_shm/)), so I couldn't increase the model size and image size as I wanted. The final solution looks like this:

- üòÄ Heavy augmentations
- üòê UNet of *medium* encoders
- üòê Medium-sized input images
- üòÄ TTA 
- üò∞ No external datasets or pseudo labels

Finally, I'd like to add some comments on the thresholding algorithm. The evaluation metric, the Dice coefficient, is very sensitive to the threshold you choose. This means that the threshold must be adjusted each time you want to evaluate the model. To make matters worse, the optimal thresholds are different for the train set (HPA) and the test set (HuBMAP). It's almost impossible to tune the threshold using the leaderboard score! To address this issue, I used an automatic thresholding algorithm called **Otsu's binarization**. Otsu's method is essentially a **discriminant analysis** for binarizing images. You can use it easily with [OpenCV](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html). In my experiments, Otsu's method performed comparable to manual tuning and better than adaptive thresholding algorithms.

![alt text](image-2.png)