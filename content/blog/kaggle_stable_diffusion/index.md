---
title: "Kaggle Competition Report: Stable Diffusion"
date: "2023-06-20T22:01:03.284Z"
description: 'HuBMAP + HPA was a competition on image segmentation with a twist in how to split the dataset. How did winners approach this problem?'
featuredImage: kaggle_stable_diffusion/ogp.jpg
tags: ["en", "kaggle"]
---

The Kaggle competition "Stable Diffusion - Image to Prompts" ended on May 15th, accommodating more than 1,000 participants.

I couldn't make it to join this competition, but I'd like to create a report post in retrospective because the winners' solutions seem to include many useful tips for multi-modal models and image generation.


## Problem Setting

The task is, in short, to invert text-to-image. That is, given an image generated by [**Stable Diffusion 2.0**](https://huggingface.co/stabilityai/stable-diffusion-2/blob/main/768-v-ema.ckpt) (**SD2**), one has to predict the prompt used to generate that image. Instead of submitting the predicted prompt, the participants must submit 384-dimensional embedding vector. The embedding model is a Sentence Transformer and it is [provided by the host](https://www.kaggle.com/datasets/inversion/sentence-transformers-222).

Submissions are evaluated using the **mean cosine similarity** score between the predicted and ground truth embedding vectors. Many people directly predicted the embedding vector, but it is also possible to predict the original prompt and then feed it to the Sentence Transformer.

Interestingly, the host provided only 7 images for training. So, the participants had to generate training data by themselves (there were no public dataset of images generated by SD2).

![](2023-06-20-07-40-15.png)

<div style="text-align: center;"><small>Image generated from a prompt: "ultrasaurus holding a black bean taco in the woods, near an identical cheneosaurus".</small></div>

## Final Standings
The final standings were quite interesting. The top team "Watercooled" is more than 0.15 ahead of the other teams in AUC. What did they do??

This competition was very stable (probably because the evaluation dataset was synthesized).

![](2023-06-20-07-43-41.png)

## Solutions

In this post, I'd like to review the top three teams' solutions.

### 1st Place

The first prize went to [@bestfitting](https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/discussion/411237). 

@bestfitting used the largest number of training samples in the top teams, creating two sets of prompts:

- PROMPT_HQ consists of 2M prompts from various sources: DiffusionDB, COCO Captions, VizWiz, ChatGPT etc. For every prompt, 2 images were generated using different random seeds. This dataset was used to finetune the models (i.e., regression of the output of Sentence Transformer).
- PROMPT_LQ is a set of 6.6M prompts selected from COYO-700M on certain criteria. This dataset was used to continue the pretraining process of the models (e.g., contrastive learning of CLIP models).

They generated images from these prompts and trained various models such as:

- **CLIP** (ViT-L)
- CLIP (ConvNeXt-XXL)
- **BLIP-2** (ViT-g + Q-Former; LLM component was removed)

These models were finetuned using **LoRA** (**low-rank adaptation**).

Generating 10 millions of images is not trivial. They used several techniques to accelerate the generation process. For example,

- [xFormers](https://github.com/facebookresearch/xformers)
- FP16 computation
- Downsizing output images

These techniques together reduced the generation time per image from 15 secs to 2 secs. When generating 10.6M images, it is equivalent to a reduction from 1840 GPU-days to 245 GPU-days (still very expensive, though).

### 2nd Place

[@zaburo](https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/discussion/410606) also relied on a large set of training samples.

The dataset was created in roughly a similar way to the one by @bestfitting, but @zaburo used a different set of models.

- CLIP (ConvNeXt-XXL)
- BLIP-2 (ViT-g + Q-Former)
- **EVA**02-L
- EVA02-e

Also, they emphasize the importance of using different random seeds every time of generation.
Linear layer


### 3rd Place

## References

[1] [Stable Diffusion - Image to Prompts - Speaker Deck](https://speakerdeck.com/kfujikawa/stable-diffusion-image-to-prompts)