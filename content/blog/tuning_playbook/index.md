---
title: 忙しい人のためのTuning Playbook
date: "2023-01-26T23:02:03.284Z"
description: "妻と続けているクリスマスの習慣「プレゼントは本にして、残った予算で寄付をする」を紹介します。"
featuredImage: tuning_playbook/ogp.jpg
tags: ["ja", "deep-learning"]
---

Tuning Playbookが公開された
https://github.com/google-research/tuning_playbook
素晴らしいものだが、例えば同僚に気軽に薦めるには長すぎる。学習も兼ねて日本語で要点をまとめた。

論文には「綺麗にされた」結果しか書かれていないが、実際には経験的に決めなければならないハイパラがたくさんある。深層学習の達人とそうでない人の間には、暗黙の知見の差が存在する。これをまとめたドキュメントを作りたい。
このドキュメントは固定化されたものではなく、常にアップデートされるものであるべき。最適解ではないかもしれないが、たたき台としては十分に役立つはずである。理論付けがなされていないものは、良い研究テーマになるかも。

# 新しくプロジェクトを始めるとき
この節では以下を仮定します：
- 問題設定やデータクリーニングは完了している
- 問題に即した適切な評価指標が選択できている
- いろいろな設定を試すための訓練と評価のパイプラインができている

1. よく使われるモデルアーキテクチャと最適化器を選ぶ
2. メモリが許す最大のバッチサイズを選ぶ
3. その他のハイパーパラメータを決める

## モデルアーキテクチャ
*既にうまくいくことがわかっているモデルから始めましょう。*
- ベースラインによく使われる、「枯れた」モデルを使いましょう
- 可能であれば類似した問題設定の論文を見つけて、それを再現実装してみましょう

## 最適化器
*その問題設定でよく使われる最適化器から始めましょう。*
- あらゆる問題に使える「最強の」最適化器はありません
  - そもそも、最適化器の性能を比較すること自体が難しい問題です
- その問題設定でよく使われる最適化器から始めましょう
  - 例：SGD（モメンタム付き）、Adam、NAdam
- 最適化器のすべてのハイパーパラメータに注意を払いましょう
  - チューニングは後段で行うので、ここでは頭の片隅に入れておくだけで構いません（詳しくはFAQ）
  - 悪い例：モデルの最適な層数を決めるときに、最適化器のハイパーパラメータをどの層数でも同じにする

## バッチサイズ
*バッチサイズは訓練が進む速度を定めるハイパーパラメータであり、検証セットでの性能によって直接チューニングすべきではありません。メモリに収まる最大値で固定しておくのがよいでしょう。*
- バッチサイズをなるべく大きく取ることで、訓練にかかる時間を最小化できます
  - 「訓練にかかる時間」は「1ステップあたりの時間」×「収束までのステップ数」です
  - 前者は、理想的にはバッチサイズによらず一定です
  - 後者は、バッチサイズと概ね反比例します
    - バッチサイズを2倍にすると、収束までのステップ数が半分になることが期待できますが、この効果はバッチサイズに対して逓減していきます
  - したがって、バッチサイズを大きく取るほど、訓練にかかる時間は短くなります
- バッチサイズを検証セットでの性能によって直接チューニングすべきではありません
  - 他のハイパーパラメータ（学習率やステップ数）を適切に選べば、任意のバッチサイズで同じ結果を達成できるはずです
- バッチサイズを決めるには、何度か訓練を実行してみる必要があります
  - OOMになるまでバッチサイズを2倍にし続けます
    - この間、1ステップあたりの時間は変わらないはずです
    - もし増えるのであれば、IOやノード間の同期などにボトルネックが存在しているので、先に進む前に解消しましょう
  - OOMになる直前のバッチサイズを選び、その値で固定します
  - 1ステップあたりの時間が一定のバッチサイズから増えるのであれば、そこで止めます
    - バッチサイズを大きくする目的は時間短縮だからです
    - 同様に、勾配累積（gradient accumulation）は時間がかかるので避けましょう
- モデルや最適化器によって許容できるバッチサイズが変わるので、これらを変更したらバッチサイズを決め直す必要があります
- 他のハイパーパラメータ、特に最適化器（学習率やモメンタム）と正則化はすべてバッチサイズに依存するので、バッチサイズは最初に決定しましょう
  - もし変更した場合は再度チューニングが必要です

## 初期設定
*初期段階のハイパーパラメータは、「そこそこの結果」にミニマルな実装で到達するという方針で決定しましょう。*
- 初期段階で定めなければならないハイパーパラメータ（のうち、ここまでで決めていないもの）は以下の3種類です
  - モデルに関するもの
    - 例：層数
  - 最適化器に関するもの
    - 例：学習率
  - 訓練ステップ数
- 初期段階では、「そこそこの結果」にミニマルな実装で到達することを目指します
  - 例：小さいモデル
  - 例：一定の学習率
- 訓練ステップ数は、性能と改善サイクルの短さのバランスから決めます

# 改善サイクルを回すとき
この節では以下を仮定します：
- 前節のプロセスによって「そこそこの結果」に到達するベースラインが得られている
- 実験を並列で実行するのに十分な計算リソースがある
 
## 全体としての心構え
*問題に対する洞察を得ることを意識しながら、確実な改善を少しずつ加えていきましょう。不必要な複雑さを避けるため、強い根拠がない限り変更を加えないようにしましょう。*
- ハイパーパラメータチューニングには自動的なチューニングアルゴリズム（Bayes最適化など）を使うことができますが、探索空間をうまく設計しないと有限の時間でいい結果を得ることができません
- 効率的に良い結果を得るためには、「自動探索は各実験（改善サイクル）の内部で利用し、そこで得られた洞察を次の実験設計に利用する」という方針を採るのが良いでしょう
  - 実験の設計を「探索」、自動探索を「活用」と捉えることもできます
  - 前者を疎かにすると局所解に陥ってしまうので、前者を重視するほうが長期的には良い結果にたどり着くことが期待できます
  - 洞察によって重要でないハイパーパラメータを特定したら、それらは固定してしまいましょう
- 全体としては以下のような手続きを繰り返すことになります
  - 次の実験の目標を決める
  - 目標を達成するための実験を設計し、実施する
  - 結果から洞察を導く
  - 満足の行く洞察が得られたら、繰り返しを終了する

## 次の目標を定める
*各実験には明確な唯一の目標を持たせましょう。*
- 実験の目標は、適切なスコープを持った明確なものであるべきです
- 良い例：
  - データの前処理方法が与える影響を調べる
  - 活性化関数の選び方が与える影響を調べる
  - 検証誤差を最小化する

## 次の実験を設計する
*scientificなハイパーパラメータを公平に比較するために、nuisanceなハイパーパラメータを最適化することでその影響を排除しましょう。*
- ハイパーパラメータは以下の3種類に分けることができます
  - scientific: 影響を調べたいもの
  - nuisance: scientificを比較するためにチューニングする必要があるもの（参考：局外パラメータ）
  - fixed: スコープを狭めるために仮定するもの。これを変えると、実験から得られる洞察が保障されなくなります
- 例：モデルの層数が性能に与える影響を調べたい場合
  - 層数はscientificです
  - 学習率はnuisanceです
  - 活性化関数はfixedです
- ハイパーパラメータの分類は実験の目標によって変わりますが、概ね以下のような方針で決めることができます
  - 最初にscientificを決め、次にリソース制約からnuisanceを決めます。残りをfixedとします。
  - 最適化器のハイパーパラメータは大抵nuisanceです。一方、最適化器の選択は大抵scientificかfixedです。
- scientificなハイパーパラメータを公平に比較するために、nuisanceなハイパーパラメータを最適化することでその影響を排除しましょう
  - ここでBayes最適化を使っても構いませんが、探索段階では準乱数に基づく方法を勧めます（詳しくはFAQ）
- リソース制約などの理由で複数のscientificなハイパーパラメータを独立に実験できない場合は、やむを得ず混ぜて実験することもあります
  - その場合は特に準乱数に基づく探索をするべきです


## 結果から洞察を導く
*結果が出て洞察を導く際には、実験が正しく実施されたか振り返るのを忘れないようにしましょう。プロットはなるべく自動化しましょう。モデルや訓練パイプラインを変更する意思決定をするときには、確率的な変動と追加される複雑さに注意しましょう。*
- 実験が正しく実施されたかどうか、以下のような観点から確認しましょう
  - 探索空間の広さは十分だったか？
    - 最適解が境界付近が見つかっているならば、探索空間を広げて実験を続けましょう
  - 探索空間を埋めるのに十分なサンプルを得られたか？
    - 不足している領域があれば、そこからサンプルを取りましょう
    - 一様に不足しているなら、実験を続けましょう
    - ただし、サンプルが十分かどうかは非自明です
  - ランタイムエラーや勾配消失が起きる試行が多数あったか？
    - そうであれば探索空間を再設計してやり直しましょう
  - 訓練曲線の形は正常か？
    - 最終的な検証誤差だけを見ていては大事な情報を見落としてしまうので、最善の数試行だけでも訓練曲線を見るようにするにしましょう
    - 過学習しているようなら、正則化（ドロップアウト、ラベル平滑化、重み減衰など）を足してみましょう
    - 訓練の後半で誤差がステップごとに振動しているなら、安定化（バッチサイズを上げる、学習率を下げるなど）を検討しましょう
- 洞察を導くには各種プロットが役に立ちます
  - 例：縦軸に検証誤差、縦軸に各ハイパーパラメータをとった散布図
  - 考察に時間を割くため、プロットの手続きは自動化しましょう
- 入れようとしている変更を反映させるかどうかの意思決定は、「改善の蓋然性」と「追加される複雑さ」のトレードオフに基づいて行います
  - 良い結果が出たとしても、それは確率的な変動による偶然かもしれません
  - 同条件で複数回試すと、意思決定の精度を上げることができます
  - それでも絶対的な確実さは得られないので、最終的には複雑さとのトレードオフから判断します

## 最後のひと押し
探索空間が決まったら、ベイズ最適化が便利
- どこかで、チューニングの問題から最高スコアを出す問題にシフトする
- ここからはベイズ最適化が強力なツール
- ここまでできたらテストセットで試してみる

# 訓練ステップ数の決め方
compute-bound: 訓練すればするだけロスが下がる
## 計算量バウンドでない場合（途中で過学習する）
- 学習が終わってから最適なチェックポイントを選べばいい
- 長くしすぎると計算が無駄にはなるが、悪影響はない
- データ拡張やドロップアウトを追加した場合、勾配の分散が増えるので、ステップ数を増やすべき
## 計算量バウンドの場合（いくらでもロスが下がる）
- 途中で止めて良し悪しを判断することはある程度可能だが、間違えるリスクはある。試行回数とのトレードオフ
- 2ラウンドでチューニングする
  - モデルと最適化器のハイパラを決める、短いランをたくさん打つ
    - ここで決めたwarmup lengthや初期化、（モデルアーキテクチャ）は次のラウンドでも有効なことが多い
    - 最適化器の選択やハイパラ、データ拡張、正則化は有効かもしれない
    - 学習率のスケジュールはおそらく有効でない
  - 学習率スケジュール
    - よくわからん

# その他
## 入力パイプラインの最適化
入力バウンドなパイプラインのボトルネックはタスクによって異なるので、プロファイラを使って検証すべき
- JAXならPerfetto、TensorFlow Profilerがある
- よくある原因
  - IO
  - データの前処理（事前にやっておくと良い）
  - マシン間の同期


## 最適なチェックポイントの選び方
retrospective optimal checkpoint selection
アーリーストッピングはしなくて良い

## 実験結果の記録
これらをメモしておく
- 実験の名前
- Configへのリンク
- 試行回数
- 最高の検証スコア
- 再現するためのコマンド
- 
## バッチ正規化
層正規化に置き換えることもよくあるが、置き換えられないときはバッチサイズやGPU数を変えるのに要注意
- 活性値の平均と分散はGPUごとに異なり、明示的に動悸されない限りは異なる値が保持される
- この値を計算するには64サンプルあれば十分

## マルチノード
バグが起きやすいので要注意！
- ログとチェックポイントは1つのノードだけでやる
- バッチ正規化が同期されている
- モデルの初期化のための乱数シードは全体で統一し、データのシャッフルや前処理の乱数シードは異なるものを使う
- データのファイルをシャードしておくと高速化できる

# FAQ
## スケジューリングはどれがいい？
最高のスケジューリングを決めるのは難しいが、何かしらは使ったほうが良い。いろいろな学習率を試すことで、モデルにとって最適な学習率に当たりやすくなる。linear decayやコサインをよく使うが、他のものでも良い。論文で見かける複雑なスケジューリングは、HIL的な手続きの結果であることもある。
## Adamのハイパラのチューニング方法は？
どこまでやるかは予算次第。10回未満で済ませるなら、学習率のみ。25回以下なら、β1も。それ以上なら、イプシロンも。もっと増やせるなら続いてβ2も。
## なぜ準ランダムな探索を推すのか？
チューニングのインサイトを得るためには（探索フェーズ）ベイズ最適化よりも適切。ランダムサーチよりもグリッドに近いため一様性が期待できる。検証エラーだけでなく訓練エラーも見たくなったときなど、後からの分析で使いやすい。統計的な再現性が高いなどのメリットが有る。
https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py
## 訓練がうまくいかないとき
不安定には2つの種類がある
- 初期
  - LRのウォームアップ（最適な学習率が、Feasibleのギリギリなとき）
    - unstableになる瞬間の10倍の学習率までウォームアップを入れる
    - どのくらいの長さでウォームアップをするかは、（10, 100, 1000, 10000）くらいのオーダーでチューニング
    - ウォームアップ後は一定の学習率で普通に学習できる
- 中盤で突然起きるもの
  - 勾配クリッピング
    - L2ノルムを監視してみる
    - 異常値が出るようだったらそこでクリップする（90%ileとか）
    - 安定化のために半分以上のステップでクリップが発生するなら、学習率を下げたほうがいい

正規化
Normalization should be the last operation before the residual. E.g. x + Norm(f(x)).
Norm(x + f(x)) known to cause issues.

学習率を下げるのは最後の手段

